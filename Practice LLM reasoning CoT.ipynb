{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use LLMs on Vector cluster\n",
    "# 1) KScope for inference: https://kscope.vectorinstitute.ai/\n",
    "# 2) VectorInference for inference and logit extraction:\n",
    "    # https://github.com/VectorInstitute/vector-inference/blob/main/examples/logits/logits.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "critical-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "generic-blowing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Load the environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "attractive-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model name (you can change the model easily).\n",
    "model=\"Meta-Llama-3.1-8B-Instruct\"\n",
    "# model=\"Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "disciplinary-degree",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Please introduce yourself.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# If model is not yet available, try again after some delay.\n",
    "output = None\n",
    "while output is None:\n",
    "    try:\n",
    "        output = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "        )\n",
    "    \n",
    "    except openai.APIError as e:\n",
    "        print(e)\n",
    "        sleep(10)\n",
    "\n",
    "print(output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fewer-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input(question):\n",
    "    \n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question,\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "systematic-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_generation_config = {\"max_tokens\": 20, \"temperature\": 0.9}\n",
    "moderate_generation_config = {\"max_tokens\": 200, \"temperature\": 0.9}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-lafayette",
   "metadata": {},
   "source": [
    "Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cordless-russell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Canada is Ottawa. It is located in the province of Ontario and is home to many\n"
     ]
    }
   ],
   "source": [
    "# message\n",
    "prompt = construct_input(\"What is the capital of Canada?\")\n",
    "generation = client.chat.completions.create(model=model, messages = prompt, **small_generation_config)\n",
    "# Extract the text from the returned generation\n",
    "print(generation.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-watson",
   "metadata": {},
   "source": [
    "# Few-Shot Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-message",
   "metadata": {},
   "source": [
    "We'll start by prompting the model to solve some word problems and build up to using the Few-Shot CoT method proposed in \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-ghana",
   "metadata": {},
   "source": [
    "First try \"zero-shot prompting\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "worse-devon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = (\n",
    "    \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? just give final answer with no explanation.\"\n",
    ")\n",
    "zero_shot_prompt = construct_input(zero_shot_prompt)\n",
    "\n",
    "generation_example = client.chat.completions.create(model=model,messages = zero_shot_prompt,  **small_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-orbit",
   "metadata": {},
   "source": [
    "The correct answer is 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-adventure",
   "metadata": {},
   "source": [
    "Now let's try standard few-shot prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "approximate-brazil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "A: The answer is 11.\n",
      "\n",
      "Q: Benjamin is taking bottle inventory. He has two cases with 15 bottles in each and one with 7. How many bottles are there in total?\n",
      "A: The answer is 37.\n",
      "\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A: The answer is ...\n",
      "Just give the final answer to the last question with no explanation.\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = (\n",
    "    \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis \"\n",
    "    \"balls does he have now?\\nA: The answer is 11.\\n\\nQ: Benjamin is taking bottle inventory. He has two cases with \"\n",
    "    \"15 bottles in each and one with 7. How many bottles are there in total?\\nA: The answer is 37.\\n\\nQ: The \"\n",
    "    \"cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\nA: \"\n",
    "    \"The answer is ...\\nJust give the final answer to the last question with no explanation.\"\n",
    ")\n",
    "few_shot_message = construct_input(few_shot_prompt)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vietnamese-islam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model,messages = few_shot_message,  **small_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-recorder",
   "metadata": {},
   "source": [
    "Now, let's try prompting the model with a few-shot CoT prompt, where we provide an example of the kind of reasoning required to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "appreciated-roads",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "few_shot_cot_prompt = (\n",
    "    \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis \"\n",
    "    \"balls does he have now?\\nA: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. \"\n",
    "    \"5 + 6 = 11. The answer is 11.\\n\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 \"\n",
    "    \"more, how many apples do they have?\\nA:\"\n",
    ")\n",
    "\n",
    "few_shot_cot_prompt_message = construct_input(few_shot_cot_prompt)\n",
    "print(few_shot_cot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "coastal-hacker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the answer, subtract the apples used from the initial number (23 - 20 = 3) and then add the apples bought (3 + 6 = 9).\n",
      "\n",
      "The final answer is 9.\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model,messages = few_shot_cot_prompt_message, **moderate_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-empire",
   "metadata": {},
   "source": [
    "## An example from the AQuA: Algebraic Word Problems task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-playing",
   "metadata": {},
   "source": [
    "Let's try to compare few-shot prompting with few-shot CoT for slightly different kind of problem. This example is drawn from the AQuA: Algebraic Word Problems task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "young-parade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\n",
      "A: The answer is (a).\n",
      "\n",
      "Q: The capacity of a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) 120000 litres (e) None of these\n",
      "A: \n",
      " what is the answer to the second Q with a,b,c,d,or e and don't say a single word except the final answer.\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = (\n",
    "    \"Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of \"\n",
    "    \"the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\\nA: The answer is (a).\\n\\nQ: The capacity of \"\n",
    "    \"a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) \"\n",
    "    \"120000 litres (e) None of these\\nA: \\n what is the answer to the second Q with a,b,c,d,or e and don't say a single word except the final answer.\"\n",
    ")\n",
    "few_shot_prompt_message = construct_input(few_shot_prompt)\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "voluntary-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(e)\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model,messages = few_shot_prompt_message, **small_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-mechanics",
   "metadata": {},
   "source": [
    "The correct choice for this problem is \"d\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "human-machine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\n",
      "A: If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean would be 50. The answer is (a).\n",
      "\n",
      "Q: The capacity of a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) 120000 litres (e) None of these \n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "few_shot_cot_prompt = (\n",
    "    \"Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers \"\n",
    "    \"is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\\nA: If 10 is added to each number, then the mean of the \"\n",
    "    \"numbers also increases by 10. So the new mean would be 50. The answer is (a).\\n\\nQ: The capacity of \"\n",
    "    \"a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) \"\n",
    "    \"120000 litres (e) None of these \\nA:\"\n",
    ")\n",
    "few_shot_cot_prompt_message = construct_input(few_shot_cot_prompt)\n",
    "print(few_shot_cot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "following-baltimore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Step 1: Calculate the volume of the tank in cubic meters.\n",
      "The volume of a rectangular tank can be calculated by multiplying its length, width, and height. In this case, the volume is given by V = length × width × height = 8 m × 6 m × 2.5 m = 120 cubic meters.\n",
      "\n",
      "## Step 2: Convert the volume from cubic meters to liters.\n",
      "Since 1 cubic meter is equal to 1000 liters, we can convert the volume of the tank by multiplying it by 1000: 120 cubic meters × 1000 liters/cubic meter = 120,000 liters.\n",
      "\n",
      "## Step 3: Identify the correct answer choice that matches the calculated volume of the tank.\n",
      "The calculated volume of the tank is 120,000 liters, which matches answer choice (d).\n",
      "\n",
      "The final answer is: $\\boxed{120000}$\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model,messages = few_shot_cot_prompt_message, **moderate_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-compilation",
   "metadata": {},
   "source": [
    "Sometimes the examples are not good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-mongolia",
   "metadata": {},
   "source": [
    "# Zero-Shot Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-commonwealth",
   "metadata": {},
   "source": [
    "It can be tedious and tricky to form useful and effective reasoning examples. Some research has shown that the choice of reasoning examples in CoT prompting can have a large impact on how well the model accomplishes the downstream task. So let's try a zero-shot CoT approach devised in \"Large Language Models are Zero-Shot Reasoners\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "disabled-vegetation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "A: The answer is 11.\n",
      "\n",
      "Q: There are 64 students trying out for the school's trivia teams. If 36 of them didn't get picked for the team and the rest were put into 4 groups, how many students would be in each group?\n",
      "A: \n",
      "Just give the final answer to the last question with no explanation.\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = (\n",
    "    \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis \"\n",
    "    \"balls does he have now?\\nA: The answer is 11.\\n\\nQ: There are 64 students trying out for the school's trivia \"\n",
    "    \"teams. If 36 of them didn't get picked for the team and the rest were put into 4 groups, how many students would \"\n",
    "    \"be in each group?\\nA: \\nJust give the final answer to the last question with no explanation.\"\n",
    ")\n",
    "\n",
    "\n",
    "few_shot_prompt_message = construct_input(few_shot_prompt)\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "martial-restoration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model,messages = few_shot_prompt_message, **small_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-hebrew",
   "metadata": {},
   "source": [
    "The correct answer to this problem is 7.\n",
    "\n",
    "Could you get the correct answer with this example?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-nation",
   "metadata": {},
   "source": [
    "# TASK: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-division",
   "metadata": {},
   "source": [
    "Try to do CoT without adding examples.\n",
    "\n",
    "\n",
    "Split into two stages:\n",
    "\n",
    "1) Reasoning Generation   \n",
    "2) Answer Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "annual-memorial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: There are 64 students trying out for the school's trivia teams.If 36 of them didn't get picked for the team and the rest were put into 4 groups, how manystudents would be in each group?\n",
      "A: \n",
      "Just give the final asnwer.\n"
     ]
    }
   ],
   "source": [
    "question_prompt = (\"Q: There are 64 students trying out for the school's trivia teams.\"\n",
    "                   \"If 36 of them didn't get picked for the team and the rest were put into 4 groups, how many\"\n",
    "                   \"students would be in each group?\\nA: \\nJust give the final asnwer.\" )\n",
    "\n",
    "question_prompt_message = construct_input(question_prompt)\n",
    "print(question_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "occupied-ethnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model, messages = question_prompt_message, **small_generation_config)\n",
    "reasoning_generation = generation_example.choices[0].message.content\n",
    "print(reasoning_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-tunisia",
   "metadata": {},
   "source": [
    "Try to get the correct answer (7) with no example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "featured-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: break down the problem into two steps. First, ask the model for reasoning,\n",
    "# then, given the reasoning, ask for the final answer by appending \"\\nTherefore, the answer is\" followed by the reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reasoning_prompt = ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flp6_kernel",
   "language": "python",
   "name": "flp6_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
