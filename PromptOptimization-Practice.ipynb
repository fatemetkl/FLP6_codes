{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wrong-public",
   "metadata": {},
   "source": [
    "# Prompt Tuning\n",
    "In this notebook, we explore prompt tuning, and see how we can easily switch between tasks with minimal fine-tuning.\n",
    "\n",
    "\n",
    "Refrences:    \n",
    "\n",
    " https://huggingface.co/docs/peft/main/en/task_guides/clm-prompt-tuning   \n",
    " https://huggingface.co/datasets/ought/raft   \n",
    " https://huggingface.co/datasets/takala/financial_phrasebank\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "finite-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-change",
   "metadata": {},
   "source": [
    "### Preprocessing function\n",
    "\n",
    "**Create a preprocess_function to:**\n",
    "\n",
    "Tokenize the input text and labels.\n",
    "\n",
    "* 1) For each example in a batch, pad the labels with the tokenizers pad_token_id.  \n",
    "* 2) Concatenate the input text and labels into the model_inputs.   \n",
    "* 3) Create a separate attention mask for labels and model_inputs.   \n",
    "* 4) Loop through each example in the batch again to pad the input ids, labels, and attention mask to the max_length and convert them to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "whole-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "        #print(i, sample_input_ids, label_input_ids)\n",
    "        \n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    #print(model_inputs)\n",
    "    \n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-carol",
   "metadata": {},
   "source": [
    "## Step1: initialize model, tokenizer, dataset, and the prompt tokens.\n",
    "The PromptTuningConfig contains information about the task type, the text to initialize the prompt embedding, the number of virtual tokens, and the tokenizer to use. PromptTuningConfig is a type of PEFT config, and in fact the training process considers the prompt weights as weights that are trained on PEFT. The rest of the training and inference is just like a regular classification with LLMs.\n",
    "PromptTuning config: https://huggingface.co/docs/peft/main/en/package_reference/prompt_tuning#peft.PromptTuningConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "shaped-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model_name_or_path = \"bigscience/bloomz-560m\"\n",
    "tokenizer_name_or_path = \"bigscience/bloomz-560m\"\n",
    "\n",
    "# ***Important***\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=8,\n",
    "    prompt_tuning_init_text=\"Classify if the tweet is a complaint or not:\",\n",
    "    tokenizer_name_or_path=model_name_or_path,\n",
    ")\n",
    "\n",
    "\n",
    "max_length = 64\n",
    "lr = 3e-2\n",
    "num_epochs = 20\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-catalyst",
   "metadata": {},
   "source": [
    "## Data loading, and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-bacon",
   "metadata": {},
   "source": [
    "Load the twitter_complaints subset of the RAFT dataset. This subset contains tweets that are labeled either complaint or no complaint:\n",
    "https://huggingface.co/datasets/ought/raft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alpine-glenn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 50, Number of test samples 3399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Tweet text': '@HMRCcustomers No this is my first job', 'ID': 0, 'Label': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"twitter_complaints\"\n",
    "text_column = \"Tweet text\"\n",
    "label_column = \"text_label\"\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"ought/raft\", dataset_name)\n",
    "print( f\"Number of training samples: {len(dataset[\"train\"])}, Number of test samples {len(dataset[\"test\"])}\")\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-craps",
   "metadata": {},
   "source": [
    "To make the Label column more readable, replace the Label value with the corresponding label text and store them in a text_label column. You can use the map function to apply this change over the entire dataset in one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exciting-assist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unlabeled', 'complaint', 'no complaint']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label 0 corrosponds to class Unlabeled, Label 1 corrosponds to complaint and Label 2 is no complaint'.\n",
    "dataset[\"train\"].features[\"Label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "binary-stanford",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tweet text': '@HMRCcustomers No this is my first job',\n",
       " 'ID': 0,\n",
       " 'Label': 2,\n",
       " 'text_label': 'no complaint'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To make the Label column more readable, replace the Label value with the corresponding label text\n",
    "classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "compressed-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "collect-constitution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([3074, 4762, 60943], 'Unlabeled'),\n",
       " ([16449, 5952], 'complaint'),\n",
       " ([1936, 106863], 'no complaint')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the number of tokens used for each class label after being tokenized.\n",
    "# The max number of tokens is 3, so the model's answers should have at most 3 tokens.\n",
    "[(tokenizer(class_label)[\"input_ids\"],class_label) for class_label in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "challenging-banana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97833f9ee7b243fba3b174a8b1425a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab03f17be1bb4ec59a8ded4c7183d38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/3399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "superb-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"test\"]\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-solution",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "Initialize a base model from AutoModelForCausalLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "illegal-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-dominant",
   "metadata": {},
   "source": [
    "### Prompt the model before training with one example: one-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "several-cathedral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Tweet text :  Categorize the following sentence in 'complaint'or 'no complaint' @nationalgridus I have no water and the bill is current and paid. Can you do something about this? Label :  No complaintThe present invention relates to a method\"]\n"
     ]
    }
   ],
   "source": [
    "# test the model before training\n",
    "inputs = tokenizer(\n",
    "    f'{text_column} : {\" Categorize the following sentence in 'complaint'or 'no complaint' @nationalgridus I have no water and the bill is current and paid. Can you do something about this?\"} Label : ',\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=2, eos_token_id=3\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-times",
   "metadata": {},
   "source": [
    "The correct answer is complaint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-sculpture",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-spirit",
   "metadata": {},
   "source": [
    "Pass the model and peft_config to the get_peft_model() function to create a PeftModel. You can print the new PeftModel’s trainable parameters to see how much more efficient it is than training the full parameters of the original model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "joint-delaware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,192 || all params: 559,222,784 || trainable%: 0.0015\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Configuring the model training with peft_config.\n",
    "model = get_peft_model(model, peft_config)\n",
    "# Trainable params are the parameters for our prompt embedding layer.\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-permit",
   "metadata": {},
   "source": [
    "### Visualize model structure and the prompt embedding layer before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "blocked-deficit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): BloomForCausalLM(\n",
       "    (transformer): BloomModel(\n",
       "      (word_embeddings): Embedding(250880, 1024)\n",
       "      (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (h): ModuleList(\n",
       "        (0-23): 24 x BloomBlock(\n",
       "          (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attention): BloomAttention(\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BloomMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu_impl): BloomGelu()\n",
       "            (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PromptEmbedding(\n",
       "      (embedding): Embedding(8, 1024)\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(250880, 1024)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "accompanied-philosophy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (default): PromptEmbedding(\n",
       "    (embedding): Embedding(8, 1024)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prompt_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "preceding-journal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding tensor values: tensor([[-0.0146, -0.0090, -0.0184,  ..., -0.0424, -0.0203,  0.0109],\n",
      "        [ 0.0189,  0.0071,  0.0161,  ..., -0.0432, -0.0030,  0.0039],\n",
      "        [-0.0062, -0.0035,  0.0085,  ..., -0.0430,  0.0081, -0.0003],\n",
      "        ...,\n",
      "        [-0.0026,  0.0100, -0.0056,  ..., -0.0434, -0.0068,  0.0047],\n",
      "        [ 0.0153,  0.0015, -0.0072,  ..., -0.0433, -0.0094, -0.0019],\n",
      "        [-0.0084,  0.0141, -0.0068,  ..., -0.0432, -0.0139,  0.0233]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "prompt_embedding_layer_before_ft = model.prompt_encoder[\"default\"].embedding.weight.data\n",
    "print(\"Embedding tensor values:\", prompt_embedding_layer_before_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-control",
   "metadata": {},
   "source": [
    "### Set the optimizer and lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "robust-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-bracelet",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "binary-cradle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.60it/s]\n",
      "100%|██████████| 425/425 [01:35<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl=tensor(239.9144, device='cuda:0') train_epoch_loss=tensor(5.4803, device='cuda:0') eval_ppl=tensor(9217.8994, device='cuda:0') eval_epoch_loss=tensor(9.1289, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.51it/s]\n",
      "100%|██████████| 425/425 [01:39<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl=tensor(176.4419, device='cuda:0') train_epoch_loss=tensor(5.1730, device='cuda:0') eval_ppl=tensor(11803.8564, device='cuda:0') eval_epoch_loss=tensor(9.3762, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.50it/s]\n",
      "100%|██████████| 425/425 [01:39<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: train_ppl=tensor(133.8714, device='cuda:0') train_epoch_loss=tensor(4.8969, device='cuda:0') eval_ppl=tensor(12261.6914, device='cuda:0') eval_epoch_loss=tensor(9.4142, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.50it/s]\n",
      "100%|██████████| 425/425 [01:39<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3: train_ppl=tensor(101.8403, device='cuda:0') train_epoch_loss=tensor(4.6234, device='cuda:0') eval_ppl=tensor(13147.6250, device='cuda:0') eval_epoch_loss=tensor(9.4840, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.51it/s]\n",
      "100%|██████████| 425/425 [01:38<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4: train_ppl=tensor(78.4906, device='cuda:0') train_epoch_loss=tensor(4.3630, device='cuda:0') eval_ppl=tensor(22318.8613, device='cuda:0') eval_epoch_loss=tensor(10.0132, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.51it/s]\n",
      "100%|██████████| 425/425 [01:38<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5: train_ppl=tensor(62.6062, device='cuda:0') train_epoch_loss=tensor(4.1369, device='cuda:0') eval_ppl=tensor(22512.9238, device='cuda:0') eval_epoch_loss=tensor(10.0218, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.52it/s]\n",
      "100%|██████████| 425/425 [01:38<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6: train_ppl=tensor(47.3318, device='cuda:0') train_epoch_loss=tensor(3.8572, device='cuda:0') eval_ppl=tensor(25957.7539, device='cuda:0') eval_epoch_loss=tensor(10.1642, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.52it/s]\n",
      " 79%|███████▉  | 336/425 [01:18<00:20,  4.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     24\u001b[0m     eval_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     25\u001b[0m     eval_preds\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m---> 26\u001b[0m         tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy(), skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     29\u001b[0m eval_epoch_loss \u001b[38;5;241m=\u001b[39m eval_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(eval_dataloader)\n\u001b[1;32m     30\u001b[0m eval_ppl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(eval_epoch_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=}  {train_epoch_loss=}\")\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    #  We can comment the below lines to make the training loop faster.\n",
    "    \n",
    "#     eval_loss = 0\n",
    "#     eval_preds = []\n",
    "#     for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**batch)\n",
    "#         loss = outputs.loss\n",
    "#         eval_loss += loss.detach().float()\n",
    "#         eval_preds.extend(\n",
    "#             tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "#         )\n",
    "\n",
    "#     eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "#     eval_ppl = torch.exp(eval_epoch_loss)\n",
    "#     print(f\"{epoch=}: {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-olympus",
   "metadata": {},
   "source": [
    "### Let's test the trained prompt with the same example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "other-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    f'{text_column} : {\"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?\"} Label : ',\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "embedded-kenya",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tweet text : @nationalgridus I have no water and the bill is current and paid. Can you do something about this? Label : complaint']\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=2, eos_token_id=3\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-mitchell",
   "metadata": {},
   "source": [
    "The correct class is \"complaint\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-letters",
   "metadata": {},
   "source": [
    "## We can keep the prompt embedding weights to pompt the model for this task\n",
    "\n",
    "Let's save the prompt embedding weights to be able to use it for the twitter_complaints task ('complaint' vs 'no complaint' prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned prompt weights\n",
    "\n",
    "prompt_embedding_weights_twitter_complaints = model.prompt_encoder[\"default\"].embedding.weight.data\n",
    "print(\"Embedding tensor values:\", prompt_embedding_weights_twitter_complaints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-shore",
   "metadata": {},
   "source": [
    "## Task 1: replace the prompt weights with zero, and see the effect on the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "painful-killer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Now replace the prompy embedding layer with zero and see the effect.\n",
    "zero_tensor = torch.zeros((8, 1024))\n",
    "zero_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ceramic-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prompt_encoder[\"default\"].embedding.weight.data = zero_tensor\n",
    "\n",
    "# Do inference again.\n",
    "inputs = tokenizer(\n",
    "    f'{text_column} : {\"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?\"} Label : ',\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=2, eos_token_id=3\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-hartford",
   "metadata": {},
   "source": [
    "Now you can see that this small portion of the model actually matters. Without fine-tuning the soft prompt layer we would need to train the whole model which would take a long time to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-pennsylvania",
   "metadata": {},
   "source": [
    "## Task 2:  Try the same model for a new task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-processing",
   "metadata": {},
   "source": [
    "You don't need to change the whole model. This is the beauty of prompt optimization.\n",
    "\n",
    "Steps to follow:\n",
    "* Load the new dataset, and follow the same steps to preprocess the data.   \n",
    "* Train a new prompt embedding matrix for the new task just like before.   \n",
    "* Save the new prompt weights. \n",
    "* Switch between the tasks by using their respective prompt weights.\n",
    "\n",
    "\n",
    "\n",
    "Don't forget to try the model with one example, before and after the training. You can use the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "unlikely-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model before and after the training\n",
    "test_inputs = tokenizer(\n",
    "    f'{text_column} : {\"The Lithuanian beer market made up 14.41 million liters in January , a rise of 0.8 percent from the year-earlier figure , the Lithuanian Brewers. Association reporting citing the results from its members .\"} Label : ',\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "# The correct answer for this example is \"Positive\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-composite",
   "metadata": {},
   "source": [
    "## New dataset and task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-saudi",
   "metadata": {},
   "source": [
    "For this task, train on the sentences_allagree subset of the financial_phrasebank dataset. This dataset contains financial news categorized by sentiment.\n",
    "https://huggingface.co/datasets/takala/financial_phrasebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "partial-steal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ad17c94a124d579a09a1606aed1232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb751b9890e496fae2685b930889e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/227 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Igor and Oleg Yankov , who currently manage Moron and Vitim , will hold onto the 25 % stake for now .',\n",
       " 'label': 1,\n",
       " 'text_label': 'neutral'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) load the dataset\n",
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "\n",
    "classes = dataset[\"train\"].features[\"label\"].names\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes[label] for label in x[\"label\"]]},\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "\n",
    "text_column = \"sentence\"\n",
    "label_column = \"text_label\"\n",
    "print( f\"Number of training samples: {len(dataset[\"train\"])}, Number of test samples {len(dataset[\"test\"])}\")\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sporting-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['negative', 'neutral', 'positive']\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels:\", dataset[\"train\"].features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer, preprocess the data, and create data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with the example before prompt optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intense-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with the example after prompt optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "typical-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the prompt weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-layout",
   "metadata": {},
   "source": [
    "## Task 3: now try switching between tasks with the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pursuant-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to the first task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it effective?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flp6_kernel",
   "language": "python",
   "name": "flp6_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
